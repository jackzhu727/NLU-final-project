{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ht_data(file_path):\n",
    "    raw = pd.read_csv(file_path, index_col = 0)\n",
    "    #raw = raw.drop(raw.columns[0], axis=1)\n",
    "    col_to_drop = ['related_topics', 'question_url', 'answers', 'sub_category']\n",
    "    raw = raw.drop(columns = col_to_drop)\n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/nlpclass/lib/python3.6/site-packages/numpy/lib/arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "full_data = read_ht_data('SINGLE_PART/healthtap_medical_qna_dataset_1.6m.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def drop_and_split(raw):\n",
    "    print(\"Length before treatment \", len(raw))\n",
    "    raw  = raw.dropna()\n",
    "    label_counter = Counter(raw['main_category'])\n",
    "    label_counter.pop('question')\n",
    "    lab = []\n",
    "    qst = []\n",
    "    for label in label_counter:\n",
    "        if label_counter[label] > 2:\n",
    "            Qs = raw[raw['main_category']==label]['question'].values.tolist()\n",
    "            qst += Qs\n",
    "            lab += len(Qs)*[label] \n",
    "    assert len(qst) == len(lab)\n",
    "    print(\"Length after treatment \", len(lab))\n",
    "    return [[inp, lab] for inp, lab in zip(qst, lab)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length before treatment  1605229\n",
      "Length after treatment  1603996\n"
     ]
    }
   ],
   "source": [
    "fdata = drop_and_split(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lab_to_ids(label):\n",
    "    lab_dict = AttrDict()\n",
    "    counter = Counter(label)\n",
    "    lab_dict.word2id = {lab:ids for (ids, lab) in enumerate(counter.keys())}\n",
    "    lab_dict.id2word = {ids:lab for lab,ids in lab_dict.word2id.items()}\n",
    "    return lab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "exclude = set(string.punctuation)\n",
    "def sent_to_words(sents):\n",
    "    def normalizeString(s):\n",
    "        s = re.sub(r\"&quot;\", r\"\", s)\n",
    "        s = re.sub(r\"&apos;\", r\"\", s)\n",
    "        s = re.sub(r\"([.!?])\", r\" \", s)\n",
    "        s = re.sub(r\"([_])\", r\" \", s)\n",
    "        s = re.sub(r\"[^a-zA-Z0-9.!?]+\", r\"\", s)\n",
    "        return s.lower()\n",
    "    def normalizeSent(sent):\n",
    "        return re.sub(' +',' ',sent).strip()\n",
    "    def sent_split(s):\n",
    "        s = re.sub(r\"/\", r\" \", s)\n",
    "        s = re.sub(r\"\\\\\", r\" \", s)\n",
    "        s = re.sub(' +',' ',s)\n",
    "        return s\n",
    "    normed = [ ' '.join([normalizeString(s) for s in sent_split(sent).split(' ') if s and s not in exclude]) for sent in sents]\n",
    "    \n",
    "    return [normalizeSent(norm).split(' ') for norm in normed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "class QuesData:\n",
    "    def __init__(self, data, sent_len_perc = 80, max_vocab_size = 25000):\n",
    "        full_input, self.MAX_sent_len = self.drop_LongSents(data, sent_len_perc)\n",
    "        self.d_inp, self.d_lab = [x[0] for x in full_input], [x[1] for x in full_input]\n",
    "        self.lab_dict = lab_to_ids(self.d_lab)\n",
    "        \n",
    "        vocab_counter = self.word_count(self.d_inp)\n",
    "        self.vocab = self.build_vocab(vocab_counter, max_vocab_size)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_sent = self.d_inp[idx]\n",
    "        labels = self.d_lab[idx]\n",
    "        input_ids = [self.vocab.word2id[word] if word in self.vocab.word2id.keys() else self.vocab.word2id['<UNK>'] for word in input_sent]\n",
    "        label_ids = [self.lab_dict.word2id[labels] ]\n",
    "\n",
    "        return input_sent, input_ids, labels, label_ids\n",
    "    \n",
    "    def drop_LongSents(self, data, sent_len_perc):\n",
    "        inp = sent_to_words([x[0] for x in data])\n",
    "        labs = [x[1] for x in data]\n",
    "        sent_lens = [len(s) for s in inp]\n",
    "        MAX_len = int(np.percentile(sent_lens, sent_len_perc))\n",
    "        dropped = [(d,lab) for d,lab in zip(inp, labs) if len(d)<=MAX_len]\n",
    "        return dropped, MAX_len\n",
    "    def word_count(self, ins):\n",
    "        count = Counter()\n",
    "        for sent in ins:\n",
    "            for word in sent:\n",
    "                if word: count[word] += 1\n",
    "        return count\n",
    "    def build_vocab(self, word_count, max_vocab_size):\n",
    "        vocab = AttrDict()\n",
    "        vocab.word2id = {'<PAD>': PAD_IDX, '<UNK>': UNK_IDX}\n",
    "        vocab.word2id.update({token: (ids + 2) for ids, (token, count) in enumerate(word_count.most_common(max_vocab_size))if count>=2 })\n",
    "        vocab.id2word = {ids:word for word, ids in vocab.word2id.items()}\n",
    "        return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cla = QuesData(fdata[:50000], sent_len_perc = 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
