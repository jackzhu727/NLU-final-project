{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "# from scipy.special import softmax # ver>=1.20\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ht_data(file_path):\n",
    "    raw = pd.read_csv(file_path, index_col = 0)\n",
    "    #raw = raw.drop(raw.columns[0], axis=1)\n",
    "    col_to_drop = ['related_topics', 'question_url', 'answers', 'sub_category']\n",
    "    raw = raw.drop(columns = col_to_drop)\n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_data = read_ht_data('healthtap_medical_qna_dataset_1.6m.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_and_split(raw):\n",
    "    # shuffle the data\n",
    "    raw = raw.reindex(np.random.permutation(raw.index))\n",
    "    raw = raw.sort_index(axis='index')\n",
    "    print(raw[:10])\n",
    "    print(\"Length before treatment \", len(raw))\n",
    "    raw  = raw.dropna()\n",
    "    label_counter = Counter(raw['main_category'])\n",
    "    label_counter.pop('question')\n",
    "    lab = []\n",
    "    qst = []\n",
    "    for label in label_counter:\n",
    "        if label_counter[label] > 2:\n",
    "            Qs = raw[raw['main_category']==label]['question'].values.tolist()\n",
    "            qst += Qs\n",
    "            lab += len(Qs)*[label] \n",
    "    assert len(qst) == len(lab)\n",
    "    print(\"Length after treatment \", len(lab))\n",
    "    return [[inp, lab] for inp, lab in zip(qst, lab)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pickle.load(open('X_train.pkl','rb'))\n",
    "y_train = pickle.load(open('y_train.pkl','rb'))\n",
    "X_test = pickle.load(open('X_test.pkl','rb'))\n",
    "y_test = pickle.load(open('y_test.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lab_to_ids(label):\n",
    "    lab_dict = AttrDict()\n",
    "    counter = Counter(label)\n",
    "    lab_dict.word2id = {lab:ids for (ids, lab) in enumerate(counter.keys())}\n",
    "    lab_dict.id2word = {ids:lab for lab,ids in lab_dict.word2id.items()}\n",
    "#     lab_dict.word2id['<UNK>'] = len(lab_dict.word2id)\n",
    "#     lab_dict.id2word[len(lab_dict.word2id)]='<UNK>'\n",
    "    return lab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "exclude = set(string.punctuation)\n",
    "def sent_to_words(sents):\n",
    "    def normalizeString(s):\n",
    "        s = re.sub(r\"&quot;\", r\"\", s)\n",
    "        s = re.sub(r\"&apos;\", r\"\", s)\n",
    "        s = re.sub(r\"([.!?])\", r\" \", s)\n",
    "        s = re.sub(r\"([_])\", r\" \", s)\n",
    "        s = re.sub(r\"[^a-zA-Z0-9.!?]+\", r\"\", s)\n",
    "        return s.lower()\n",
    "    def normalizeSent(sent):\n",
    "        return re.sub(' +',' ',sent).strip()\n",
    "    def sent_split(s):\n",
    "        s = re.sub(r\"/\", r\" \", s)\n",
    "        s = re.sub(r\"\\\\\", r\" \", s)\n",
    "        s = re.sub(' +',' ',s)\n",
    "        return s\n",
    "    normed = [ ' '.join([normalizeString(s) for s in sent_split(sent).split(' ') if s and s not in exclude]) for sent in sents]\n",
    "    \n",
    "    return [normalizeSent(norm).split(' ') for norm in normed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trans_mat():\n",
    "    df = pd.read_csv(\"symptoms-DO.tsv\",sep = '\\t')\n",
    "    d_set = list(df['disease_name'].unique())\n",
    "    s_set = list(df['symptom_name'].unique())\n",
    "    tran_mat = np.full((len(s_set), len(d_set)), -np.inf)\n",
    "    for i, s_name in enumerate(s_set):\n",
    "        d_subset = df[df['symptom_name'] == s_name]['disease_name'].unique()\n",
    "        js = [d_set.index(x) for x in d_subset]\n",
    "        tran_mat[i,js] = 1\n",
    "    tran_mat = torch.from_numpy(tran_mat).to(device)\n",
    "    return torch.softmax(tran_mat, dim=1), s_set, d_set\n",
    "tran_mat, row_name, col_name = get_trans_mat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "class QuesData(Dataset):\n",
    "    def __init__(self, sent, cat, sent_len_perc = 80, max_vocab_size = 25000, valid_vocab=None):\n",
    "        full_input, self.MAX_sent_len = self.drop_LongSents(sent, cat, sent_len_perc)\n",
    "#         full_input, self.MAX_sent_len = self.drop_LongSents(data, sent_len_perc)\n",
    "        self.d_inp, self.d_lab = [x[0] for x in full_input], [x[1] for x in full_input]\n",
    "        if not valid_vocab:\n",
    "            self.lab_dict = lab_to_ids(self.d_lab)\n",
    "            vocab_counter = self.word_count(self.d_inp)\n",
    "            self.vocab = self.build_vocab(vocab_counter, max_vocab_size)\n",
    "        else:\n",
    "            self.lab_dict = valid_vocab['label']\n",
    "            self.vocab = valid_vocab['input']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.d_inp)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_sent = self.d_inp[idx]\n",
    "        labels = self.d_lab[idx]\n",
    "        input_ids = [self.vocab.word2id[word] if word in self.vocab.word2id.keys() else self.vocab.word2id['<UNK>'] for word in input_sent]\n",
    "        label_ids = [self.lab_dict.word2id[labels]]\n",
    "#         label_ids = [self.lab_dict.word2id[label] if label in self.lab_dict.word2id.keys() else self.lab_dict.word2id['<UNK>'] for label in labels]\n",
    "        return input_ids, label_ids[0]\n",
    "#         return input_sent, input_ids, labels, label_ids\n",
    "    \n",
    "    def drop_LongSents(self, sent, labs, sent_len_perc):\n",
    "        inp = sent_to_words(sent)\n",
    "#         labs = [x[1] for x in data]\n",
    "        sent_lens = [len(s) for s in inp]\n",
    "        print(max(sent_lens))\n",
    "        MAX_len = int(np.percentile(sent_lens, sent_len_perc))\n",
    "        dropped = [(d,lab) for d,lab in zip(inp, labs) if len(d)<=MAX_len]\n",
    "        return dropped, MAX_len\n",
    "    def word_count(self, ins):\n",
    "        count = Counter()\n",
    "        for sent in ins:\n",
    "            for word in sent:\n",
    "                if word: count[word] += 1\n",
    "        return count\n",
    "    def build_vocab(self, word_count, max_vocab_size):\n",
    "        vocab = AttrDict()\n",
    "        vocab.word2id = {'<PAD>': PAD_IDX, '<UNK>': UNK_IDX}\n",
    "        vocab.word2id.update({token: (ids + 2) for ids, (token, count) in enumerate(word_count.most_common(max_vocab_size))if count>=2 })\n",
    "        vocab.id2word = {ids:word for word, ids in vocab.word2id.items()}\n",
    "        return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batchify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    def _pad_sequences(seqs):\n",
    "        lens = [len(seq) for seq in seqs]\n",
    "        padded_seqs = torch.zeros(len(seqs), max(lens))\n",
    "        for i, seq in enumerate(seqs):\n",
    "            end = lens[i]\n",
    "            padded_seqs[i, :end] = torch.LongTensor(seq[:end]).to(device)\n",
    "        return padded_seqs, lens\n",
    "\n",
    "    data.sort(key=lambda x: len(x[0]), reverse=True) #sort according to length of src seqs\n",
    "    ques_seqs, trg = zip(*data)\n",
    "    ques_seqs, ques_lens = _pad_sequences(ques_seqs)\n",
    "    #(batch, seq_len) => (seq_len, batch)\n",
    "    ques_seqs.transpose_(0,1)\n",
    "\n",
    "    return ques_seqs, ques_lens, trg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "773\n",
      "804\n"
     ]
    }
   ],
   "source": [
    "# with open('preproc_data.pkl','rb') as file:\n",
    "#     fdata = pickle.load(file)\n",
    "BATCH_SIZE = 128\n",
    "train_dataset = QuesData(X_train, y_train, sent_len_perc = 85)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=collate_fn,\n",
    "                                           shuffle=True)\n",
    "valid_dataset = QuesData(X_test, y_test, sent_len_perc = 85, valid_vocab={'input':train_dataset.vocab, 'label':train_dataset.lab_dict})\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=collate_fn,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model with BiLSTM Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size,num_classes, embed_size, row_name, col_name, tran_mat, hidden_size,n_layers = 1, dropout=0):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "        self.embedding = nn.Embedding(input_size, embed_size, padding_idx=PAD_IDX)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, n_layers, dropout=self.dropout, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size,num_classes)\n",
    "        \n",
    "    def forward(self, input_seqs, input_lengths, hidden=None):\n",
    "        batch_size = input_seqs.size()[1]\n",
    "        embedded = self.embedding(input_seqs) #input_seq: T*B, embedded: T*B*H\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        _, hidden = self.lstm(packed, hidden)\n",
    "        hidden = torch.cat((hidden[0][0,:,:],hidden[0][1,:,:]), dim = 1)\n",
    "        out = torch.relu(self.fc1(hidden))\n",
    "        return self.fc2(out)\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(2, 2, BATCH_SIZE, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input, label, in_lens, model, optim, criterion, max_clip_norm = 5):\n",
    "    input = input.long().to(device)\n",
    "    label = torch.LongTensor(label).to(device)\n",
    "    in_lens = torch.LongTensor(in_lens).to(device)\n",
    "    batch_size = input.size(1)\n",
    "    model.train()\n",
    "    optim.zero_grad()\n",
    "#     hidden = model.initHidden()\n",
    "    logits = model(input, in_lens)\n",
    "    loss = criterion(logits, label)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_clip_norm)\n",
    "    optim.step()\n",
    "    return loss.item()\n",
    "def freeze_layer(layer):\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_iter):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for input, in_lens, label in data_iter:\n",
    "        input = input.long().to(device)\n",
    "        label = torch.LongTensor(label).to(device)\n",
    "        in_lens = torch.LongTensor(in_lens).to(device)\n",
    "        output = model(input, in_lens)\n",
    "        \n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += label.size(0)\n",
    "        correct += (predicted == label).sum().item()\n",
    "      \n",
    "    return correct / float(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_size = len(train_dataset.vocab.id2word)\n",
    "num_classes = len(train_dataset.lab_dict.id2word)\n",
    "embed_size = 100\n",
    "hidden_size = 100\n",
    "model = EncoderLSTM(input_size,num_classes, embed_size, row_name, col_name, tran_mat, hidden_size,n_layers = 1, dropout=0).to(device)\n",
    "model.load_state_dict(torch.load('lstm01.pth'))\n",
    "model.train()\n",
    "# freeze_layer(model.lstm)\n",
    "# freeze_layer(model.embedding)\n",
    "encoder_optim = optim.Adam([p for p in model.parameters() if p.requires_grad], lr=1e-3, weight_decay=1e-10)\n",
    "encoder_scheduler = torch.optim.lr_scheduler.LambdaLR(encoder_optim, lr_lambda=lambda epoch: 0.95 ** epoch)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "accs = []\n",
    "train_loss = []\n",
    "count = 1\n",
    "for epoch in range(30):\n",
    "    total_loss = 0\n",
    "    for idx, batch_data in enumerate(train_loader):\n",
    "        train_input, train_lens, train_label = batch_data\n",
    "        loss = train(train_input, train_label, train_lens, model, encoder_optim, criterion, 2)\n",
    "        total_loss += loss\n",
    "        if idx%1000 == 0:\n",
    "            print('Training Loss: {}'.format(loss))\n",
    "\n",
    "    \n",
    "    train_loss.append((total_loss/(idx+ 1)))\n",
    "    if epoch%1==0:\n",
    "        train_acc = evaluate(model, train_loader)\n",
    "        val_acc = evaluate(model, valid_loader)\n",
    "        print(\"Epoch %i; Train acc: %f; Dev acc %f\" %(epoch,\\\n",
    "                        train_acc, val_acc))\n",
    "        try:\n",
    "            if val_acc > max(accs):\n",
    "                torch.save(model.state_dict(), 'lstm01.pth')\n",
    "        except:\n",
    "            torch.save(model.state_dict(), 'lstm01.pth')\n",
    "        accs.append(val_acc)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
